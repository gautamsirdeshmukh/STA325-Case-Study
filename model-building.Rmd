---
title: "STA 325 Case Study"
authors: "Matthew Cui, Phillip Harmadi, Glen Morgenstern, Joe Wang, Gaurav Sirdeshmukh, Gautam Sirdeshmukh"
output: pdf_document
---

```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Load libraries and data
```{r echo=FALSE, message=FALSE}
library(tidyverse)
library(car)
library(boot)
library(gam)
library(splines)
train <- read_csv("data-train.csv")
test <- read_csv("data-test.csv")
train
cor(train)
test
```

## Exploratory Data Analysis

```{r eda}
# We transform the variables using the sigmoid function so that this variable 
# will be within a finite range.

train1 <- train %>%
  rename(M1 = R_moment_1, M2 = R_moment_2, M3 = R_moment_3, M4 = R_moment_4) %>%
  mutate(Fr_sigmoid = 1 / (1 + exp(-Fr)),
         Re_sigmoid = 1 / (1 + exp(-Re)),
         M1_sigmoid = 1 / (1 + exp(-M1)),
         M2_sigmoid = 1 / (1 + exp(-M2)),
         M3_sigmoid = 1 / (1 + exp(-M3)),
         M4_sigmoid = 1 / (1 + exp(-M4)))

train1
cor(train1)

test1 <- test %>%
  mutate(Fr_sigmoid = 1 / (1 + exp(-Fr)),
         Re_sigmoid = 1 / (1 + exp(-Re)))

test1
```

```{r, fig.height = 2, fig.width = 3}
library(scales)
par(mfrow = c(5,3))
ggplot(data = train1, mapping = aes(x = St)) + 
  geom_histogram(fill = "skyblue", color = "white") + 
  scale_x_continuous(expand = c(0,0)) +
  scale_y_continuous(expand = c(0,0)) +
  theme_minimal()
ggsave("./plots/eda1.png", plot = last_plot(), width = 4.5, height = 3)
ggplot(data = train1, mapping = aes(x = Re)) + 
  geom_histogram(fill = "skyblue", color = "white") +
  scale_x_continuous(expand = c(0,0)) +
  scale_y_continuous(expand = c(0,0)) +
  theme_minimal()
ggsave("./plots/eda2.png", plot = last_plot(), width = 4.5, height = 3)
ggplot(data = train1, mapping = aes(x = Fr)) + 
  geom_histogram(fill = "skyblue", color = "white") +
  scale_x_continuous(expand = c(0,0)) +
  scale_y_continuous(expand = c(0,0)) +
  theme_minimal()
ggsave("./plots/eda3.png", plot = last_plot(), width = 4.5, height = 3)
ggplot(data = train1, mapping = aes(x = Fr_sigmoid)) + 
  geom_histogram(fill = "skyblue", color = "white") +
  scale_x_continuous(expand = c(0,0)) +
  scale_y_continuous(expand = c(0,0)) +
  theme_minimal()
ggsave("./plots/eda4.5.png", plot = last_plot(), width = 4.5, height = 3)
ggplot(data = train1, mapping = aes(x = M1)) + 
  geom_histogram(fill = "skyblue", color = "white") +
  scale_x_continuous(expand = c(0,0)) +
  scale_y_continuous(expand = c(0,0)) +
  theme_minimal()
ggsave("./plots/eda5.png", plot = last_plot(), width = 4.5, height = 3)
ggplot(data = train1, mapping = aes(x = M1_sigmoid)) + 
  geom_histogram(fill = "skyblue", color = "white") +
  scale_x_continuous(expand = c(0,0)) +
  scale_y_continuous(expand = c(0,0)) +
  theme_minimal()
ggsave("./plots/eda6.png", plot = last_plot(), width = 4.5, height = 3)
ggplot(data = train1, mapping = aes(x = M2)) + 
  geom_histogram(fill = "skyblue", color = "white") +
  scale_x_continuous(expand = c(0,0)) +
  scale_y_continuous(expand = c(0,0)) +
  theme_minimal()
ggsave("./plots/eda7.png", plot = last_plot(), width = 4.5, height = 3)
ggplot(data = train1, mapping = aes(x = M2_sigmoid)) + 
  geom_histogram(fill = "skyblue", color = "white") +
  scale_x_continuous(expand = c(0,0)) +
  scale_y_continuous(expand = c(0,0)) +
  theme_minimal()
ggsave("./plots/eda8.png", plot = last_plot(), width = 4.5, height = 3)
ggplot(data = train1, mapping = aes(x = M3)) + 
  geom_histogram(fill = "skyblue", color = "white") +
  scale_x_continuous(expand = c(0,0), labels = label_number(suffix = "M", scale = 1e-6)) +
  scale_y_continuous(expand = c(0,0)) +
  theme_minimal()
ggsave("./plots/eda9.png", plot = last_plot(), width = 4.5, height = 3)
ggplot(data = train1, mapping = aes(x = M3_sigmoid)) + 
  geom_histogram(fill = "skyblue", color = "white") +
  scale_x_continuous(expand = c(0,0)) +
  scale_y_continuous(expand = c(0,0)) +
  theme_minimal()
ggsave("./plots/eda10.png", plot = last_plot(), width = 4.5, height = 3)
ggplot(data = train1, mapping = aes(x = M4)) + 
  geom_histogram(fill = "skyblue", color = "white") +
  scale_x_continuous(expand = c(0,0), labels = label_number(suffix = "B", scale = 1e-9)) +
  scale_y_continuous(expand = c(0,0)) +
  theme_minimal()
ggsave("./plots/eda11.png", plot = last_plot(), width = 4.5, height = 3)
ggplot(data = train1, mapping = aes(x = M4_sigmoid)) + 
  geom_histogram(fill = "skyblue", color = "white") +
  scale_x_continuous(expand = c(0,0)) +
  scale_y_continuous(expand = c(0,0)) +
  theme_minimal()
ggsave("./plots/eda12.png", plot = last_plot(), width = 4.5, height = 3)
ggplot(data = train1, mapping = aes(x = log(St), y = M1)) + 
  geom_point() +
  theme_minimal()
ggsave("./plots/eda13.png", plot = last_plot(), width = 4.5, height = 3)
train1 <- train1 %>%
  mutate(Re_categorical = case_when(Re == 90 ~ "Low", Re == 224 ~ "Medium", Re == 398 ~ "High"),
         Fr_categorical = case_when(Fr == 0.052 ~ "Low", Fr == 0.3 ~ "Medium", Fr == Inf ~ "High"))
ggplot(data = train1, mapping = aes(x = St, y = M2)) + 
  geom_point() +
  theme_minimal()
ggsave("./plots/eda14.png", plot = last_plot(), width = 4.5, height = 3)
ggplot(data = train1, mapping = aes(x = St*Re*Fr_sigmoid, y = M2)) + 
  geom_point() +
  theme_minimal()
ggsave("./plots/eda15.png", plot = last_plot(), width = 4.5, height = 3)
```

We will try to create these 4 models:

- **Response**: M1 & **Predictors (Main Effects)**: St, Re, Fr_sigmoid

We will attempt to use a combination of subset selection, polynomial, 
transformation, and interaction variables.

- **Response**: M2 & **Predictors (Main Effects)**: St, Re, Fr_sigmoid, M1

We will attempt to use a combination of subset selection, polynomial, 
transformation, and interaction variables. We will also include M1 since
it is has significant positive relationship with M2 (~0.63).

- **Response**: M3 & **Predictors (Main Effects)**: M2

We know that M2 is almost perfectly correlated (>0.99) with M3, 
so only using one predictor variable is enough. We try to avoid overfitting by
using only M2 as our only predictor to predict M3. We will
attempt to use polynomial and transformation variables.

- **Response**: M4 & **Predictors (Main Effects)**: M2, M3

Same reasoning - M2 and M3 are almost perfectly correlated
with M4. We will only use these 2 predictors and will attempt to use
both transformation and interaction variables (since M2 and M3
are also highly correlated to each other).

## Predictive models

```{r predictive-model-1a, warning = FALSE}
# Model 1a
model_1a <- lm(log(M1) ~ St + Re_categorical + Fr_categorical, data = train1)
summary(model_1a)

# 5-fold Cross Validation
set.seed(200)
cv_error_5_1a = rep(0, 5)
for (i in 1:5) {
  model_1a <- glm(log(M1) ~ St + Re_categorical + Fr_categorical, data = train1)
  cv_error_5_1a[i] = cv.glm(train1, model_1a, K = 5)$delta[2]
  }
sum(cv_error_5_1a) / 5 # MSE
```


```{r predictive-model-1b, warning = FALSE}
# Model 1b
model_1b <- lm(log(M1) ~ St + Re_categorical + Fr_categorical + 
                St * Re_categorical + St * Fr_categorical + Re_categorical * Fr_categorical, 
              data = train1)
summary(model_1b)

# 10-fold Cross Validation
set.seed(100)
cv_error_10_1b = rep(0, 10)
for (i in 1:10) {
  model_1b <- glm(M1 ~ St + Re_categorical + Fr_categorical + 
                St * Re_categorical + St * Fr_categorical + Re_categorical * Fr_categorical, 
              data = train1)
  cv_error_10_1b[i] = cv.glm(train1, model_1b, K = 10)$delta[1]
  }
sum(cv_error_10_1b) / 10 # MSE
```

```{r, fig.width = 3, fig.height = 2}
par(mfrow = c(2,2))

ggplot(data = train1, mapping = aes(x = Re, y = log(M1), color = Fr_categorical)) + 
  geom_point(alpha = 0.5) + 
  geom_smooth(size = 0.5, se = FALSE) + 
  scale_y_continuous(limits = c(-8, -2)) +
  labs(x = "Re",
       y = "Logarithm of M1",
       color = "Fr") +
  theme_minimal(base_size = 11)
ggsave("./plots/eda_1.png", plot = last_plot(), width = 4.5, height = 3)

ggplot(data = train1, mapping = aes(x = Re, y = log(M2), color = Fr_categorical)) + 
  geom_point(alpha = 0.5) + 
  geom_smooth(size = 0.5, se = FALSE) + 
  scale_y_continuous(limits = c(-10, 10)) +
  labs(x = "Re",
       y = "Logarithm of M2",
       color = "Fr") +
  theme_minimal(base_size = 11) 
ggsave("./plots/eda_2.png", plot = last_plot(), width = 4.5, height = 3)

ggplot(data = train1, mapping = aes(x = Re, y = log(M3), color = Fr_categorical)) + 
  geom_point(alpha = 0.5) + 
  geom_smooth(size = 0.5, se = FALSE) + 
  scale_y_continuous(limits = c(-10, 20)) +
  labs(x = "Re",
       y = "Logarithm of M3",
       color = "Fr") +
  theme_minimal(base_size = 11)
ggsave("./plots/eda_3.png", plot = last_plot(), width = 4.5, height = 3)

ggplot(data = train1, mapping = aes(x = Re, y = log(M4), color = Fr_categorical)) + 
  geom_point(alpha = 0.5) + 
  geom_smooth(size = 0.5, se = FALSE) + 
  scale_y_continuous(limits = c(-15, 30)) +
    labs(x = "Re",
       y = "Logarithm of M4",
       color = "Fr") +
  theme_minimal(base_size = 11)
ggsave("./plots/eda_4.png", plot = last_plot(), width = 4.5, height = 3)
```

```{r}
ggplot(data = train1, mapping = aes(x = St, y = Re)) + 
  geom_point(alpha = 0.5) + 
  geom_smooth(size = 0.5, se = FALSE) +
  labs(x = "St",
       y = "Re") +
  theme_minimal(base_size = 11)
ggsave("./plots/eda16.png", plot = last_plot(), width = 4.5, height = 3)

ggplot(data = train1, mapping = aes(x = St, y = log(M1), color = Fr_categorical)) + 
  geom_point(alpha = 0.5) + 
  geom_smooth(size = 0.5, se = FALSE) +
  labs(x = "St",
       y = "Logarithm of M1",
       color = "Fr") +
  theme_minimal(base_size = 11)
ggsave("./plots/eda17.png", plot = last_plot(), width = 4.5, height = 3)

ggplot(data = train1, mapping = aes(x = St, y = log(M2), color = Fr_categorical)) + 
  geom_point(alpha = 0.5) + 
  geom_smooth(size = 0.5, se = FALSE) +
  labs(x = "St",
       y = "Logarithm of M2",
       color = "Fr") +
  theme_minimal(base_size = 11)
ggsave("./plots/eda18.png", plot = last_plot(), width = 4.5, height = 3)

ggplot(data = train1, mapping = aes(x = St, y = log(M3), color = Fr_categorical)) + 
  geom_point(alpha = 0.5) + 
  geom_smooth(size = 0.5, se = FALSE) +
  labs(x = "St",
       y = "Logarithm of M3",
       color = "Fr") +
  theme_minimal(base_size = 11)
ggsave("./plots/eda19.png", plot = last_plot(), width = 4.5, height = 3)

ggplot(data = train1, mapping = aes(x = St, y = log(M4), color = Fr_categorical)) + 
  geom_point(alpha = 0.5) + 
  geom_smooth(size = 0.5, se = FALSE) +
  labs(x = "St",
       y = "Logarithm of M4",
       color = "Fr") +
  theme_minimal(base_size = 11)
ggsave("./plots/eda20.png", plot = last_plot(), width = 4.5, height = 3)
```



```{r predictive-model-1c, warning = FALSE}
# Model 1c
model_1c <- glm(M1_sigmoid ~ St + Re_categorical + Fr_categorical, data = train1)
# summary(model1c)
with(summary(model_1c), 1 - deviance / null.deviance) # R^2

# 10-fold Cross Validation
set.seed(100)
cv_error_10_1c = rep(0, 10)
for (i in 1:10) {
  model_1c <- glm(M1_sigmoid ~ St + Re_categorical + Fr_categorical, data = train1)
  cv_error_10_1c[i] = cv.glm(train1, model_1c, K = 10)$delta[1]
  }
sum(cv_error_10_1c) / 10 # MSE
```

```{r predictive-model-1d, warning = FALSE}
# Model 1d
model_1d <- glm(M1_sigmoid ~ St + Re_categorical + Fr_categorical +
                St * Re_categorical + St * Fr_categorical + Re_categorical * Fr_categorical, 
                data = train1)
# summary(model1d)
with(summary(model_1d), 1 - deviance / null.deviance) # R^2

# 10-fold Cross Validation
set.seed(100)
cv_error_5_1d = rep(0, 5)
for (i in 1:5) {
  model_1d <- glm(M1_sigmoid ~ St + Re_categorical + Fr_categorical + 
                St * Re_categorical + St * Fr_categorical + Re_categorical * Fr_categorical, 
              data = train1)
  cv_error_5_1d[i] = cv.glm(train1, model_1d, K = 5)$delta[1]
  }
sum(cv_error_5_1d) / 5 # MSE
```

```{r m1-final-model}
# Model 1e, final model
model_1e <- lm(log(M1) ~ poly(St, 3) + Re_categorical + Fr_categorical, data = train1)
summary(model_1e)

# 5-fold Cross Validation
final_df <- tibble(seed = "", poly = "", cv_error_5_1e = "")
for (j in c(1:100)) {
set.seed(j)
cv_error_5_1e = rep(NA, 10)
for (i in 1:10) {
  model_1e <- glm(log(M1) ~ poly(St, 7) + Re_categorical + Fr_categorical, data = train1)
  cv_error_5_1e[i] = cv.glm(train1, model_1e, K = 5)$delta[2]
}
seed <- rep(j, 10)
poly <- c(1:10)
cv_df <- data.frame(seed, poly, cv_error_5_1e)
final_df <- rbind(final_df, cv_df)
}


final_df <- final_df %>%
  mutate(seed = as.factor(seed),
         poly = as.double(poly),
         cv_error_5_1e = as.double(cv_error_5_1e)) %>%
  na.omit()
final_df

final_summary <- final_df %>%
  group_by(poly) %>%
  summarize(cv_error_5_1e = mean(cv_error_5_1e)) %>%
  mutate(seed = 0)
final_summary

final <- rbind(final_summary, final_df)
final

ggplot() +
  geom_line(data = final %>% filter(seed > 0), 
            mapping = aes(x = poly, y = cv_error_5_1e, color = seed, alpha = 0.5)) +
  scale_color_manual(values = c(rep("black", 100))) +
  geom_line(data = final %>% filter(seed == 0),
            mapping = aes(x = poly, y = cv_error_5_1e, alpha = 1), color = "black") +
  labs(title = "CV Error (All Iterations)",
       y = "CV Error",
       x = "Polynomial Degree",
       caption = "The thick line is the average across all CV iterations (thin lines).") +
  scale_x_continuous(breaks = seq(1, 11, by = 1)) +
  scale_y_continuous() +
  theme_minimal(base_size = 10) +
  theme(legend.position = "none",
        panel.grid.minor = element_blank(),
        plot.caption = element_text(hjust = 0.5))
ggsave("./plots/cv_err_model1.png", plot = last_plot(), width = 6, height = 4.5)
```


```{r m1-final-model}
poly <- c(1:10)
cv_df <- data.frame(poly, cv_error_5_1e)

ggplot(final_summary, aes(x = poly, y = cv_error_5_1e)) +
  geom_line() +
  scale_x_continuous(breaks = c(1:10)) +
  labs(title = "CV Error (Average)",
       y = "CV Error (Average)",
       x = "Polynomial Degree",
       caption = "We zoomed in to focus only on the thick line - the average CV for all 100 iterations.") +
  theme_minimal(base_size = 10) +
  theme(plot.caption = element_text(hjust = 0.5))
ggsave("./plots/cv_err_model1_average.png", plot = last_plot(), width = 6, height = 4)
```

## M2

```{r predictive-models warning = F, message=F}
# Model 2 (linear)
model2 <- lm(log(M2) ~ St + Re_categorical + Fr_categorical, data = train1)
summary(model2)
# vif(model2)

# Model 2 interactions
model2_int1 <- lm(log(M2) ~ St + Re_categorical + Fr_categorical + 
                    Re_categorical * Fr_categorical, data = train1)
summary(model2_int1)
#vif(model2_int1)

# 5-fold Cross Validation
set.seed(200)
cv_error_5_2inter = rep(0, 5)
for (i in 1:5) {
  model2_int1 <- glm(log(M2) ~ St + Re_categorical + Fr_categorical + Re_categorical * Fr_categorical, data = train1)
  cv_error_5_2inter[i] = cv.glm(train1, model2_int1, K = 5)$delta[2]
  }
sum(cv_error_5_2inter) / 5 # MSE

# Model 2 polynomial
model2_int1poly <- lm(log(M2) ~ poly(St, 10) + Re_categorical + Fr_categorical +
                        Re_categorical * Fr_categorical, data = train1)
summary(model2_int1poly)

# 5-fold Cross Validation
set.seed(200)
cv_error_5_2poly = rep(0, 5)
for (i in 1:5) {
  model2_int1poly <- glm(log(M2) ~ poly(St, 10) + Re_categorical + Fr_categorical + Re_categorical * Fr_categorical, data = train1)
  cv_error_5_2poly[i] = cv.glm(train1, model2_int1poly, K = 5)$delta[2]
  }
sum(cv_error_5_2poly) / 5 # MSE
```

The polynomial model has a much lower CV error (0.54) than the linear model with interactions but no polynomial (CV error of 1.65). However, spline has an even lower CV error, at around 0.45.

```{r m2-polynomial}
set.seed(301)
cv_rss = rep(NA, 10)
for (i in 1:10) {
    glm.fit = glm(log(M2) ~ poly(St, i) + Fr_categorical + Re_categorical + Re_categorical * Fr_categorical, data = train1)
    cv_rss[i] = cv.glm(train1, glm.fit, K = 5)$delta[2]
}

poly <- c(1:10)
cv_df <- data.frame(poly, cv_rss)

ggplot(cv_df, aes(x = poly, y = cv_rss)) +
  geom_line() +
  scale_x_continuous(breaks = c(1:10)) +
  labs(title = "CV Error",
       y = "CV Error",
       x = "Polynomial Degree") +
  theme_minimal(base_size = 11)

best_poly <- rep(0, length(1:30))
```


```{r m2-polynomial}
final_df <- tibble(seed = "", poly = "", cv_rss = "")
final_df

# for (j in c(1:30)) {
# set.seed(j)
# cv_rss = rep(NA, 10)
# for (i in 1:10) {
#     glm.fit = glm(log(M2) ~ poly(St, i) + Fr_categorical + Re_categorical + Re_categorical * Fr_categorical, data = train1)
#     cv_rss[i] = cv.glm(train1, glm.fit, K = 5)$delta[2]
# }
# 
# seed <- rep(j, 10)
# poly <- c(1:10)
# cv_df <- data.frame(seed, poly, cv_rss)
# final_df <- rbind(final_df, cv_df)
# # best_poly[j] <- which.min(x = cv_df$cv_rss)
# }
# # summary(best_poly)
# # hist(best_poly)
# final_df
```

```{r}
final_df <- final_df %>%
  mutate(seed = as.factor(seed),
         poly = as.double(poly),
         cv_rss = as.double(cv_rss)) %>%
  na.omit()

final_summary <- final_df %>%
  group_by(poly) %>%
  summarize(cv_rss = mean(cv_rss)) %>%
  mutate(seed = 0)

final <- rbind(final_summary, final_df)
final


ggplot() +
  geom_line(data = final %>% filter(seed > 0), 
            mapping = aes(x = poly, y = cv_rss, color = seed, alpha = 0.5)) +
  scale_color_manual(values = c(rep("black", 30))) +
  geom_line(data = final %>% filter(seed == 0),
            mapping = aes(x = poly, y = cv_rss, alpha = 1), color = "black") +
  labs(title = "CV Error",
       y = "CV Error",
       x = "Polynomial Degree",
       caption = "The thick line is the average across all CV iterations (thin lines).") +
  scale_x_continuous(breaks = seq(1, 11, by = 1)) +
  scale_y_continuous(breaks = seq(0, 2, by = 0.25)) +
  theme_minimal(base_size = 11) +
  theme(legend.position = "none",
        panel.grid.minor = element_blank(),
        plot.caption = element_text(hjust = 0.5))
ggsave("./plots/cv_err_model2.png", plot = last_plot(), width = 6, height = 4.5)
``` 

## M3

```{r}
# Model 3 (linear)
model3 <- lm(log(M3) ~ St + Re_categorical + Fr_categorical, data = train1)
summary(model3)

model3_int1 <- lm(log(M3) ~ St + Re_categorical + Fr_categorical + 
                    Re_categorical * Fr_categorical, data = train1)
summary(model3_int1)

# 5-fold Cross Validation
set.seed(200)
cv_error_5_3inter = rep(0, 5)
for (i in 1:5) {
  model3_int1 <- glm(log(M3) ~ St + Re_categorical + Fr_categorical + Re_categorical * Fr_categorical, data = train1)
  cv_error_5_3inter[i] = cv.glm(train1, model3_int1, K = 5)$delta[2]
  }
sum(cv_error_5_3inter) / 5 # MSE
```

``` {r}
# Model 3 polynomial
model3_int1poly <- lm(log(M3) ~ poly(St, 10) + Re_categorical + Fr_categorical +
                        Re_categorical * Fr_categorical, data = train1)
summary(model3_int1poly)

# 5-fold Cross Validation
set.seed(200)
cv_error_5_3poly = rep(0, 5)
for (i in 1:5) {
  model3_int1poly <- glm(log(M3) ~ poly(St, 10) + Re_categorical + Fr_categorical + Re_categorical * Fr_categorical, data = train1)
  cv_error_5_3poly[i] = cv.glm(train1, model3_int1poly, K = 5)$delta[2]
  }
sum(cv_error_5_3poly) / 5 # MSE
```

```{r}
best_poly <- rep(0, length(1:30))
for (j in 1:30) {
set.seed(j)
cv_rss = rep(NA, 10)
for (i in 1:10) {
    glm.fit = glm(log(M3) ~ poly(St, i) + Fr_categorical + Re_categorical + Re_categorical * Fr_categorical, data = train1)
    cv_rss[i] = cv.glm(train1, glm.fit, K = 5)$delta[2]
}

poly <- c(1:10)
cv_df <- data.frame(poly, cv_rss)

best_poly[j] <- which.min(x = cv_df$cv_rss)
}
summary(best_poly)
hist(best_poly)
```

Like M3, the spline model (with about 22 degrees of freedom) produces the lowest CV error, at about 1.4 This is slightly lower than the polynomial of degree 10 (CV error of 1.6) and much lower than the basic linear model with one interaction term (CV error of 4.4). 

## M4
```{r}
# Model 4 (linear)
model4 <- lm(log(M4) ~ St + Re_categorical + Fr_categorical, data = train1)
summary(model4)

model4_int1 <- lm(log(M4) ~ St + Re_categorical + Fr_categorical + 
                    Re_categorical * Fr_categorical, data = train1)
summary(model4_int1)
```


# Nonlinear models

```{r}
cv_rss = rep(NA, 10)
for (i in 1:10) {
    glm.fit = glm(log(M1) ~ poly(St, i) + Fr_categorical + Re_categorical, data = train1)
    cv_rss[i] = cv.glm(train1, glm.fit, K = 5)$delta[2]
}

poly <- c(1:10)
cv_df <- data.frame(poly, cv_rss)

cv_df

ggplot(cv_df, aes(x = poly, y = cv_rss)) +
  geom_line() +
  scale_x_continuous(breaks = c(1:10)) +
  labs(title = "CV Error",
       y = "CV Error",
       x = "Polynomial Degree") +
  theme_minimal(base_size = 11)
```

### Repeat for 30 seeds to see most common optimal polynomial degree
```{r}
best_poly <- rep(0, length(1:30))
for (j in 1:30) {
set.seed(j)
cv_rss = rep(NA, 10)
for (i in 1:10) {
    glm.fit = glm(log(M1) ~ poly(St, i) + Fr_categorical + Re_categorical, data = train1)
    cv_rss[i] = cv.glm(train1, glm.fit, K = 5)$delta[2]
}

poly <- c(1:10)
cv_df <- data.frame(poly, cv_rss)

# ggplot(cv_df, aes(x = poly, y = cv_rss)) +
#   geom_line() +
#   scale_x_continuous(breaks = c(1:10)) +
#   labs(title = "CV Error",
#        y = "CV Error",
#        x = "Polynomial Degree") +
#   theme_minimal(base_size = 11)

best_poly[j] <- which.min(x = cv_df$cv_rss)
}
summary(best_poly)
hist(best_poly)
```

## Spline M1
```{r warning = FALSE}
df_range_cv = rep(NA, 25)
for (i in 3:25) {
    lm.fit = gam(log(M1) ~ s(St, df = i) + Re_categorical + Fr_categorical, data = train1)
    df_range_cv[i] = cv.glm(train1, lm.fit, K = 5)$delta[2]
}

spline_df_cv <- c(3:25)
rss_cv <- df_range_cv[-c(1, 2)]
spline_rss_cv <- data.frame(spline_df_cv, rss_cv)
spline_rss_cv

ggplot(spline_rss_cv, aes(x = spline_df_cv, y = rss_cv)) +
  geom_line() +
  scale_x_continuous() +
  labs(title = "Spline",
       y = "CV Error",
       x = "df") +
  theme_minimal(base_size = 11)
```

## Spline M2
```{r}
final_df <- tibble(seed = "", df = "", cv_rss_m2 = "")

for (j in c(1:10)) {
set.seed(j)
cv_rss_m2 = rep(NA, 15)

for (i in 1:15) {
    lm.fit = gam(log(M2) ~ s(St, df = i) + Re_categorical + Fr_categorical + 
                   Re_categorical * Fr_categorical, data = train1)
    df_range_cv[i] = cv.glm(train1, lm.fit, K = 5)$delta[2]
}

seed <- rep(j, 15)
df <- c(1:15)
cv_rss_m2 <- df_range_cv
cv_df <- data.frame(seed, df)
final_df <- rbind(final_df, cv_df)
}

final_df

```

```{r}
final_df
final_df <- final_df %>%
  mutate(seed = as.factor(seed),
         df = as.double(df),
         cv_rss = as.double(cv_rss_m2)) %>%
  na.omit()

final_summary <- final_df %>%
  group_by(df) %>%
  summarize(cv_rss_m2 = mean(cv_rss_m2)) %>%
  mutate(seed = 0)

final <- rbind(final_summary, final_df)
final


ggplot() +
  geom_line(data = final %>% filter(seed > 0), 
            mapping = aes(x = df, y = cv_rss_m2, color = seed, alpha = 0.5)) +
  scale_color_manual(values = c(rep("black", 35))) +
  geom_line(data = final %>% filter(seed == 0),
            mapping = aes(x = df, y = cv_rss_m2, alpha = 1), color = "black") +
  labs(title = "CV Error",
       y = "CV Error",
       x = "Degrees of Freedom") +
  scale_x_continuous(breaks = seq(1, 35, by = 2)) +
  scale_y_continuous(breaks = seq(0, 2, by = 0.25)) +
  theme_minimal(base_size = 11) +
  theme(legend.position = "none",
        panel.grid.minor = element_blank())
ggsave("./plots/cv_err_model2_spline.png", plot = last_plot(), width = 6, height = 4.5)
```


## Spline M3

```{r}
final_df <- tibble(seed = "", df = "", cv_rss = "")
final_df

for (j in c(1:10)) {
set.seed(j)
cv_rss = rep(NA, 25)

for (i in 1:25) {
    lm.fit = gam(log(M3) ~ s(St, df = i) + Re_categorical + Fr_categorical + 
                   Re_categorical * Fr_categorical, data = train1)
    df_range_cv[i] = cv.glm(train1, lm.fit, K = 5)$delta[2]
}

seed <- rep(j, 25)
df <- c(1:25)
cv_rss <- df_range_cv
cv_df <- data.frame(seed, df, cv_rss)
final_df <- rbind(final_df, cv_df)
}
```

```{r}
# df_range_cv = rep(NA, 25)
# for (i in 1:25) {
#     lm.fit = gam(log(M3) ~ s(St, df = i) + Re_categorical + Fr_categorical + 
#                    Re_categorical * Fr_categorical, data = train1)
#     df_range_cv[i] = cv.glm(train1, lm.fit, K = 5)$delta[2]
# }
# 
# spline_df_cv <- c(1:25)
# rss_cv <- df_range_cv
# spline_rss_cv <- data.frame(spline_df_cv, rss_cv)
# spline_rss_cv
# 
# ggplot(spline_rss_cv, aes(x = spline_df_cv, y = rss_cv)) +
#   geom_line() +
#   scale_x_continuous() +
#   labs(title = "Spline",
#        y = "CV Error",
#        x = "df") +
#   theme_minimal(base_size = 11)
```

```{r}
final_df
final_df <- final_df %>%
  mutate(seed = as.factor(seed),
         df = as.double(df),
         cv_rss = as.double(cv_rss)) %>%
  na.omit()

final_summary <- final_df %>%
  group_by(df) %>%
  summarize(cv_rss = mean(cv_rss)) %>%
  mutate(seed = 0)

final <- rbind(final_summary, final_df)
final


ggplot() +
  geom_line(data = final %>% filter(seed > 0), 
            mapping = aes(x = df, y = cv_rss, color = seed, alpha = 0.5)) +
  scale_color_manual(values = c(rep("black", 35))) +
  geom_line(data = final %>% filter(seed == 0),
            mapping = aes(x = df, y = cv_rss, alpha = 1), color = "black") +
  labs(title = "CV Error",
       y = "CV Error",
       x = "Degrees of Freedom") +
  scale_x_continuous(breaks = seq(1, 35, by = 2)) +
  scale_y_continuous(breaks = seq(0, 2, by = 0.25)) +
  theme_minimal(base_size = 11) +
  theme(legend.position = "none",
        panel.grid.minor = element_blank())
ggsave("./plots/cv_err_model2_spline.png", plot = last_plot(), width = 6, height = 4.5)
```

## Apply to test data

```{r test-data}

```


---
title: "STA 325 Case Study"
authors: "Matthew Cui, Phillip Harmadi, Glen Morgenstern, Joe Wang, Gaurav Sirdeshmukh, Gautam Sirdeshmukh"
output: pdf_document
---

```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Load libraries and data
```{r echo=FALSE, message=FALSE}
library(tidyverse)
library(car)
library(boot)
train <- read_csv("data-train.csv")
test <- read_csv("data-test.csv")
train
cor(train)
test
```

## Exploratory Data Analysis

```{r eda}
# We transform the variables using the sigmoid function so that this variable 
# will be within a finite range.

train1 <- train %>%
  rename(M1 = R_moment_1, M2 = R_moment_2, M3 = R_moment_3, M4 = R_moment_4) %>%
  mutate(Fr_sigmoid = 1 / (1 + exp(-Fr)),
         Re_sigmoid = 1 / (1 + exp(-Re)),
         M1_sigmoid = 1 / (1 + exp(-M1)),
         M2_sigmoid = 1 / (1 + exp(-M2)),
         M3_sigmoid = 1 / (1 + exp(-M3)),
         M4_sigmoid = 1 / (1 + exp(-M4)))

train1
cor(train1)

test1 <- test %>%
  mutate(Fr_sigmoid = 1 / (1 + exp(-Fr)),
         Re_sigmoid = 1 / (1 + exp(-Re)))

test1
```

```{r}
ggplot(data = train1, mapping = aes(x = St)) + geom_histogram()
ggplot(data = train1, mapping = aes(x = Re)) + geom_histogram()
ggplot(data = train1, mapping = aes(x = Fr)) + geom_histogram()
ggplot(data = train1, mapping = aes(x = Fr_sigmoid)) + geom_histogram()
ggplot(data = train1, mapping = aes(x = M1)) + geom_histogram()
ggplot(data = train1, mapping = aes(x = M1_sigmoid)) + geom_histogram()
ggplot(data = train1, mapping = aes(x = M2)) + geom_histogram()
ggplot(data = train1, mapping = aes(x = M2_sigmoid)) + geom_histogram()
ggplot(data = train1, mapping = aes(x = M3)) + geom_histogram()
ggplot(data = train1, mapping = aes(x = M3_sigmoid)) + geom_histogram()
ggplot(data = train1, mapping = aes(x = M4)) + geom_histogram()
ggplot(data = train1, mapping = aes(x = M4_sigmoid)) + geom_histogram()

ggplot(data = train1, mapping = aes(x = log(St), y = M1)) + geom_point()

train1 <- train1 %>%
  mutate(Re_categorical = case_when(Re == 90 ~ "Low", Re == 224 ~ "Medium", Re == 398 ~ "High"),
         Fr_categorical = case_when(Fr == 0.052 ~ "Low", Fr == 0.3 ~ "Medium", Fr == Inf ~ "High"))
ggplot(data = train1, mapping = aes(x = St, y = M2)) + geom_point()
ggplot(data = train1, mapping = aes(x = St*Re*Fr_sigmoid, y = M2)) + geom_point()
```

We will try to create these 4 models:

- **Response**: M1 & **Predictors (Main Effects)**: St, Re, Fr_sigmoid

We will attempt to use a combination of subset selection, polynomial, 
transformation, and interaction variables.

- **Response**: M2 & **Predictors (Main Effects)**: St, Re, Fr_sigmoid, M1

We will attempt to use a combination of subset selection, polynomial, 
transformation, and interaction variables. We will also include M1 since
it is has significant positive relationship with M2 (~0.63).

- **Response**: M3 & **Predictors (Main Effects)**: M2

We know that M2 is almost perfectly correlated (>0.99) with M3, 
so only using one predictor variable is enough. We try to avoid overfitting by
using only M2 as our only predictor to predict M3. We will
attempt to use polynomial and transformation variables.

- **Response**: M4 & **Predictors (Main Effects)**: M2, M3

Same reasoning - M2 and M3 are almost perfectly correlated
with M4. We will only use these 2 predictors and will attempt to use
both transformation and interaction variables (since M2 and M3
are also highly correlated to each other).

## Predictive models

```{r predictive-model-1a, warning = FALSE}
# Model 1a
model_1a <- lm(log(M1) ~ St + Re_categorical + Fr_categorical, data = train1)
summary(model_1a)

# 10-fold Cross Validation
set.seed(100)
cv_error_10_1a = rep(0, 10)
for (i in 1:10) {
  model_1a <- glm(M1 ~ St + Re_categorical + Fr_categorical, data = train1)
  cv_error_10_1a[i] = cv.glm(train1, model_1a, K = 10)$delta[1]
  }
sum(cv_error_10_1a) / 10 # MSE
```

```{r}
cv_rss = rep(NA, 10)
for (i in 1:10) {
    glm.fit = glm(M1 ~ poly(St, i) + Re_categorical + Fr_categorical, data = train1)
    cv_rss[i] = cv.glm(train1, glm.fit, K = 10)$delta[2]
}

poly <- c(1:10)
cv_df <- data.frame(poly, cv_rss)

ggplot(cv_df, aes(x = poly, y = cv_rss)) +
  geom_line() +
  scale_x_continuous(breaks = c(1:10)) +
  labs(title = "CV Error",
       y = "CV Error",
       x = "Polynomial Degree") +
  theme_minimal(base_size = 11)
```


```{r predictive-model-1b, warning = FALSE}
# Model 1b
model_1b <- lm(log(M1) ~ St + Re_categorical + Fr_categorical + 
                St * Re_categorical + St * Fr_categorical + Re_categorical * Fr_categorical, 
              data = train1)
summary(model_1b)

# 10-fold Cross Validation
set.seed(100)
cv_error_10_1b = rep(0, 10)
for (i in 1:10) {
  model_1b <- glm(M1 ~ St + Re_categorical + Fr_categorical + 
                St * Re_categorical + St * Fr_categorical + Re_categorical * Fr_categorical, 
              data = train1)
  cv_error_10_1b[i] = cv.glm(train1, model_1b, K = 10)$delta[1]
  }
sum(cv_error_10_1b) / 10 # MSE
```

```{r}
ggplot(data = train1, mapping = aes(x = Re, y = log(M1), color = Fr_categorical)) + geom_point() + geom_smooth() + theme_minimal(base_size = 11)
ggplot(data = train1, mapping = aes(x = Re, y = log(M2), color = Fr_categorical)) + geom_point() + geom_smooth() + theme_minimal(base_size = 11)
ggplot(data = train1, mapping = aes(x = Re, y = log(M3), color = Fr_categorical)) + geom_point() + geom_smooth() + theme_minimal(base_size = 11)
ggplot(data = train1, mapping = aes(x = Re, y = log(M4), color = Fr_categorical)) + geom_point() + geom_smooth() + theme_minimal(base_size = 11)
```

```{r}
ggplot(data = train1, mapping = aes(x = St, y = Re)) + geom_point()
ggplot(data = train1, mapping = aes(x = St, y = log(M1), color = Fr_categorical)) + geom_point() + geom_smooth() + theme_minimal(base_size = 11)
ggplot(data = train1, mapping = aes(x = St, y = log(M2), color = Fr_categorical)) + geom_point() + geom_smooth() + theme_minimal(base_size = 11)
ggplot(data = train1, mapping = aes(x = St, y = log(M3), color = Fr_categorical)) + geom_point() + geom_smooth() + theme_minimal(base_size = 11)
ggplot(data = train1, mapping = aes(x = St, y = log(M4), color = Fr_categorical)) + geom_point() + geom_smooth() + theme_minimal(base_size = 11)
```


```{r predictive-model-1c, warning = FALSE}
# Model 1c
model_1c <- glm(M1_sigmoid ~ St + Re_categorical + Fr_categorical, data = train1)
# summary(model1c)
with(summary(model_1c), 1 - deviance / null.deviance) # R^2

# 10-fold Cross Validation
set.seed(100)
cv_error_10_1c = rep(0, 10)
for (i in 1:10) {
  model_1c <- glm(M1_sigmoid ~ St + Re_categorical + Fr_categorical, data = train1)
  cv_error_10_1c[i] = cv.glm(train1, model_1c, K = 10)$delta[1]
  }
sum(cv_error_10_1c) / 10 # MSE
```

```{r predictive-model-1d, warning = FALSE}
# Model 1d
model_1d <- glm(M1_sigmoid ~ St + Re_categorical + Fr_categorical +
                St * Re_categorical + St * Fr_categorical + Re_categorical * Fr_categorical, 
                data = train1)
# summary(model1d)
with(summary(model_1d), 1 - deviance / null.deviance) # R^2

# 10-fold Cross Validation
set.seed(100)
cv_error_10_1d = rep(0, 10)
for (i in 1:10) {
  model_1d <- glm(M1_sigmoid ~ St + Re_categorical + Fr_categorical + 
                St * Re_categorical + St * Fr_categorical + Re_categorical * Fr_categorical, 
              data = train1)
  cv_error_10_1d[i] = cv.glm(train1, model_1d, K = 10)$delta[1]
  }
sum(cv_error_10_1d) / 10 # MSE
```
## M2

```{r predictive-models}
# Model 2 (linear)
model2 <- lm(log(M2) ~ St + Re_categorical + Fr_categorical, data = train1)
summary(model2)
# vif(model2)

# Model 2 interactions
model2_int1 <- lm(log(M2) ~ St + Re_categorical + Fr_categorical + 
                    Re_categorical * Fr_categorical, data = train1)
summary(model2_int1)
#vif(model2_int1)
```

## M3

```{r}
# Model 3 (linear)
model3 <- lm(log(M3) ~ St + Re_categorical + Fr_categorical, data = train1)
summary(model3)

model3_int1 <- lm(log(M3) ~ St + Re_categorical + Fr_categorical + 
                    Re_categorical * Fr_categorical, data = train1)
summary(model3_int1)
```

## M4
```{r}
# Model 4 (linear)
model4 <- lm(log(M4) ~ St + Re_categorical + Fr_categorical, data = train1)
summary(model4)

model4_int1 <- lm(log(M4) ~ St + Re_categorical + Fr_categorical + 
                    Re_categorical * Fr_categorical, data = train1)
summary(model4_int1)
```

## Apply to test data

```{r test-data}

```


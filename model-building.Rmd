---
title: "STA 325 Case Study"
authors: "Matthew Cui, Phillip Harmadi, Glen Morgenstern, Joe Wang, Gaurav Sirdeshmukh, Gautam Sirdeshmukh"
output: pdf_document
---

```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Load libraries and data
```{r echo=FALSE, message=FALSE}
library(tidyverse)
library(car)
library(boot)
library(gam)
library(splines)
train <- read_csv("data-train.csv")
test <- read_csv("data-test.csv")
train
cor(train)
test
```

## Exploratory Data Analysis

```{r eda}
# We transform the variables using the sigmoid function so that this variable 
# will be within a finite range.

train1 <- train %>%
  rename(M1 = R_moment_1, M2 = R_moment_2, M3 = R_moment_3, M4 = R_moment_4) %>%
  mutate(Fr_sigmoid = 1 / (1 + exp(-Fr)),
         Re_sigmoid = 1 / (1 + exp(-Re)),
         M1_sigmoid = 1 / (1 + exp(-M1)),
         M2_sigmoid = 1 / (1 + exp(-M2)),
         M3_sigmoid = 1 / (1 + exp(-M3)),
         M4_sigmoid = 1 / (1 + exp(-M4)))

train1
cor(train1)

test1 <- test %>%
  mutate(Fr_sigmoid = 1 / (1 + exp(-Fr)),
         Re_sigmoid = 1 / (1 + exp(-Re)))

test1
```

```{r}
ggplot(data = train1, mapping = aes(x = St)) + geom_histogram()
ggplot(data = train1, mapping = aes(x = Re)) + geom_histogram()
ggplot(data = train1, mapping = aes(x = Fr)) + geom_histogram()
ggplot(data = train1, mapping = aes(x = Fr_sigmoid)) + geom_histogram()
ggplot(data = train1, mapping = aes(x = M1)) + geom_histogram()
ggplot(data = train1, mapping = aes(x = M1_sigmoid)) + geom_histogram()
ggplot(data = train1, mapping = aes(x = M2)) + geom_histogram()
ggplot(data = train1, mapping = aes(x = M2_sigmoid)) + geom_histogram()
ggplot(data = train1, mapping = aes(x = M3)) + geom_histogram()
ggplot(data = train1, mapping = aes(x = M3_sigmoid)) + geom_histogram()
ggplot(data = train1, mapping = aes(x = M4)) + geom_histogram()
ggplot(data = train1, mapping = aes(x = M4_sigmoid)) + geom_histogram()

ggplot(data = train1, mapping = aes(x = log(St), y = M1)) + geom_point()

train1 <- train1 %>%
  mutate(Re_categorical = case_when(Re == 90 ~ "Low", Re == 224 ~ "Medium", Re == 398 ~ "High"),
         Fr_categorical = case_when(Fr == 0.052 ~ "Low", Fr == 0.3 ~ "Medium", Fr == Inf ~ "High"))
ggplot(data = train1, mapping = aes(x = St, y = M2)) + geom_point()
ggplot(data = train1, mapping = aes(x = St*Re*Fr_sigmoid, y = M2)) + geom_point()
```

We will try to create these 4 models:

- **Response**: M1 & **Predictors (Main Effects)**: St, Re, Fr_sigmoid

We will attempt to use a combination of subset selection, polynomial, 
transformation, and interaction variables.

- **Response**: M2 & **Predictors (Main Effects)**: St, Re, Fr_sigmoid, M1

We will attempt to use a combination of subset selection, polynomial, 
transformation, and interaction variables. We will also include M1 since
it is has significant positive relationship with M2 (~0.63).

- **Response**: M3 & **Predictors (Main Effects)**: M2

We know that M2 is almost perfectly correlated (>0.99) with M3, 
so only using one predictor variable is enough. We try to avoid overfitting by
using only M2 as our only predictor to predict M3. We will
attempt to use polynomial and transformation variables.

- **Response**: M4 & **Predictors (Main Effects)**: M2, M3

Same reasoning - M2 and M3 are almost perfectly correlated
with M4. We will only use these 2 predictors and will attempt to use
both transformation and interaction variables (since M2 and M3
are also highly correlated to each other).

## Predictive models

```{r predictive-model-1a, warning = FALSE}
# Model 1a
model_1a <- lm(log(M1) ~ St + Re_categorical + Fr_categorical, data = train1)
summary(model_1a)

# 5-fold Cross Validation
set.seed(200)
cv_error_5_1a = rep(0, 5)
for (i in 1:5) {
  model_1a <- glm(log(M1) ~ St + Re_categorical + Fr_categorical, data = train1)
  cv_error_5_1a[i] = cv.glm(train1, model_1a, K = 5)$delta[2]
  }
sum(cv_error_5_1a) / 5 # MSE
```


```{r predictive-model-1b, warning = FALSE}
# Model 1b
model_1b <- lm(log(M1) ~ St + Re_categorical + Fr_categorical + 
                St * Re_categorical + St * Fr_categorical + Re_categorical * Fr_categorical, 
              data = train1)
summary(model_1b)

# 10-fold Cross Validation
set.seed(100)
cv_error_10_1b = rep(0, 10)
for (i in 1:10) {
  model_1b <- glm(M1 ~ St + Re_categorical + Fr_categorical + 
                St * Re_categorical + St * Fr_categorical + Re_categorical * Fr_categorical, 
              data = train1)
  cv_error_10_1b[i] = cv.glm(train1, model_1b, K = 10)$delta[1]
  }
sum(cv_error_10_1b) / 10 # MSE
```

```{r, fig.width = 3, fig.height = 2}
par(mfrow = c(2,2))

ggplot(data = train1, mapping = aes(x = Re, y = log(M1), color = Fr_categorical)) + 
  geom_point(alpha = 0.5) + 
  geom_smooth(size = 0.5, se = FALSE) + 
  scale_y_continuous(limits = c(-8, -2)) +
  labs(x = "Re",
       y = "Logarithm of M1",
       color = "Fr") +
  theme_minimal(base_size = 11)
ggplot(data = train1, mapping = aes(x = Re, y = log(M2), color = Fr_categorical)) + 
  geom_point(alpha = 0.5) + 
  geom_smooth(size = 0.5, se = FALSE) + 
  scale_y_continuous(limits = c(-10, 10)) +
  labs(x = "Re",
       y = "Logarithm of M2",
       color = "Fr") +
  theme_minimal(base_size = 11) 
ggplot(data = train1, mapping = aes(x = Re, y = log(M3), color = Fr_categorical)) + 
  geom_point(alpha = 0.5) + 
  geom_smooth(size = 0.5, se = FALSE) + 
  scale_y_continuous(limits = c(-10, 20)) +
  labs(x = "Re",
       y = "Logarithm of M3",
       color = "Fr") +
  theme_minimal(base_size = 11)
ggplot(data = train1, mapping = aes(x = Re, y = log(M4), color = Fr_categorical)) + 
  geom_point(alpha = 0.5) + 
  geom_smooth(size = 0.5, se = FALSE) + 
  scale_y_continuous(limits = c(-15, 30)) +
    labs(x = "Re",
       y = "Logarithm of M4",
       color = "Fr") +
  theme_minimal(base_size = 11)
```

```{r}
ggplot(data = train1, mapping = aes(x = St, y = Re)) + geom_point()
ggplot(data = train1, mapping = aes(x = St, y = log(M1), color = Fr_categorical)) + geom_point() + geom_smooth() + theme_minimal(base_size = 11)
ggplot(data = train1, mapping = aes(x = St, y = log(M2), color = Fr_categorical)) + geom_point() + geom_smooth() + theme_minimal(base_size = 11)
ggplot(data = train1, mapping = aes(x = St, y = log(M3), color = Fr_categorical)) + geom_point() + geom_smooth() + theme_minimal(base_size = 11)
ggplot(data = train1, mapping = aes(x = St, y = log(M4), color = Fr_categorical)) + geom_point() + geom_smooth() + theme_minimal(base_size = 11)
```


```{r predictive-model-1c, warning = FALSE}
# Model 1c
model_1c <- glm(M1_sigmoid ~ St + Re_categorical + Fr_categorical, data = train1)
# summary(model1c)
with(summary(model_1c), 1 - deviance / null.deviance) # R^2

# 10-fold Cross Validation
set.seed(100)
cv_error_10_1c = rep(0, 10)
for (i in 1:10) {
  model_1c <- glm(M1_sigmoid ~ St + Re_categorical + Fr_categorical, data = train1)
  cv_error_10_1c[i] = cv.glm(train1, model_1c, K = 10)$delta[1]
  }
sum(cv_error_10_1c) / 10 # MSE
```

```{r predictive-model-1d, warning = FALSE}
# Model 1d
model_1d <- glm(M1_sigmoid ~ St + Re_categorical + Fr_categorical +
                St * Re_categorical + St * Fr_categorical + Re_categorical * Fr_categorical, 
                data = train1)
# summary(model1d)
with(summary(model_1d), 1 - deviance / null.deviance) # R^2

# 10-fold Cross Validation
set.seed(100)
cv_error_5_1d = rep(0, 5)
for (i in 1:5) {
  model_1d <- glm(M1_sigmoid ~ St + Re_categorical + Fr_categorical + 
                St * Re_categorical + St * Fr_categorical + Re_categorical * Fr_categorical, 
              data = train1)
  cv_error_5_1d[i] = cv.glm(train1, model_1d, K = 5)$delta[1]
  }
sum(cv_error_5_1d) / 5 # MSE
```

```{r m1-final-model}
# Model 1e, final model
model_1e <- lm(log(M1) ~ poly(St, 7) + Re_categorical + Fr_categorical, data = train1)
summary(model_1e)

# 5-fold Cross Validation
set.seed(200)
cv_error_5_1e = rep(0, 5)
for (i in 1:5) {
  model_1e <- glm(log(M1) ~ poly(St, 7) + Re_categorical + Fr_categorical, data = train1)
  cv_error_5_1e[i] = cv.glm(train1, model_1e, K = 5)$delta[2]
  }
sum(cv_error_5_1e) / 5 # MSE
```

## M2

```{r predictive-models warning = F, message=F}
# Model 2 (linear)
model2 <- lm(log(M2) ~ St + Re_categorical + Fr_categorical, data = train1)
summary(model2)
# vif(model2)

# Model 2 interactions
model2_int1 <- lm(log(M2) ~ St + Re_categorical + Fr_categorical + 
                    Re_categorical * Fr_categorical, data = train1)
summary(model2_int1)
#vif(model2_int1)

# 5-fold Cross Validation
set.seed(200)
cv_error_5_2inter = rep(0, 5)
for (i in 1:5) {
  model2_int1 <- glm(log(M2) ~ St + Re_categorical + Fr_categorical + Re_categorical * Fr_categorical, data = train1)
  cv_error_5_2inter[i] = cv.glm(train1, model2_int1, K = 5)$delta[2]
  }
sum(cv_error_5_2inter) / 5 # MSE

# Model 2 polynomial
model2_int1poly <- lm(log(M2) ~ poly(St, 10) + Re_categorical + Fr_categorical +
                        Re_categorical * Fr_categorical, data = train1)
summary(model2_int1poly)

# 5-fold Cross Validation
set.seed(200)
cv_error_5_2poly = rep(0, 5)
for (i in 1:5) {
  model2_int1poly <- glm(log(M2) ~ poly(St, 10) + Re_categorical + Fr_categorical + Re_categorical * Fr_categorical, data = train1)
  cv_error_5_2poly[i] = cv.glm(train1, model2_int1poly, K = 5)$delta[2]
  }
sum(cv_error_5_2poly) / 5 # MSE
```

The polynomial model has a much lower CV error (0.54) than the linear model with interactions but no polynomial (CV error of 1.65). However, spline has an even lower CV error, at around 0.45.

```{r m2-polynomial}
set.seed(301)
cv_rss = rep(NA, 10)
for (i in 1:10) {
    glm.fit = glm(log(M2) ~ poly(St, i) + Fr_categorical + Re_categorical + Re_categorical * Fr_categorical, data = train1)
    cv_rss[i] = cv.glm(train1, glm.fit, K = 5)$delta[2]
}

poly <- c(1:10)
cv_df <- data.frame(poly, cv_rss)

ggplot(cv_df, aes(x = poly, y = cv_rss)) +
  geom_line() +
  scale_x_continuous(breaks = c(1:10)) +
  labs(title = "CV Error",
       y = "CV Error",
       x = "Polynomial Degree") +
  theme_minimal(base_size = 11)

best_poly <- rep(0, length(1:30))
```


```{r m2-polynomial}
final_df <- tibble(seed = "", poly = "", cv_rss = "")
final_df

for (j in c(1:30)) {
set.seed(j)
cv_rss = rep(NA, 11)
for (i in 1:11) {
    glm.fit = glm(log(M2) ~ poly(St, i) + Fr_categorical + Re_categorical + Re_categorical * Fr_categorical, data = train1)
    cv_rss[i] = cv.glm(train1, glm.fit, K = 5)$delta[2]
}

seed <- rep(j, 11)
poly <- c(1:11)
cv_df <- data.frame(seed, poly, cv_rss)
final_df <- rbind(final_df, cv_df)

best_poly[j] <- which.min(x = cv_df$cv_rss)
}
summary(best_poly)
hist(best_poly)
final_df
```

```{r}
final_df <- final_df %>%
  mutate(seed = as.factor(seed),
         poly = as.double(poly),
         cv_rss = as.double(cv_rss)) %>%
  na.omit()

final_summary <- final_df %>%
  group_by(poly) %>%
  summarize(cv_rss = mean(cv_rss)) %>%
  mutate(seed = 0)

final <- rbind(final_summary, final_df)
final


ggplot() +
  geom_line(data = final %>% filter(seed > 0), 
            mapping = aes(x = poly, y = cv_rss, color = seed, alpha = 0.5)) +
  scale_color_manual(values = c(rep("black", 30))) +
  geom_line(data = final %>% filter(seed == 0),
            mapping = aes(x = poly, y = cv_rss, alpha = 1), color = "black") +
  labs(title = "CV Error",
       y = "CV Error",
       x = "Polynomial Degree") +
  scale_x_continuous(breaks = seq(1, 11, by = 1)) +
  scale_y_continuous(breaks = seq(0, 2, by = 0.25)) +
  theme_minimal(base_size = 11) +
  theme(legend.position = "none",
        panel.grid.minor = element_blank())
``` 

## M3

```{r}
# Model 3 (linear)
model3 <- lm(log(M3) ~ St + Re_categorical + Fr_categorical, data = train1)
summary(model3)

model3_int1 <- lm(log(M3) ~ St + Re_categorical + Fr_categorical + 
                    Re_categorical * Fr_categorical, data = train1)
summary(model3_int1)

# 5-fold Cross Validation
set.seed(200)
cv_error_5_3inter = rep(0, 5)
for (i in 1:5) {
  model3_int1 <- glm(log(M3) ~ St + Re_categorical + Fr_categorical + Re_categorical * Fr_categorical, data = train1)
  cv_error_5_3inter[i] = cv.glm(train1, model3_int1, K = 5)$delta[2]
  }
sum(cv_error_5_3inter) / 5 # MSE
```

``` {r}
# Model 3 polynomial
model3_int1poly <- lm(log(M3) ~ poly(St, 10) + Re_categorical + Fr_categorical +
                        Re_categorical * Fr_categorical, data = train1)
summary(model3_int1poly)

# 5-fold Cross Validation
set.seed(200)
cv_error_5_3poly = rep(0, 5)
for (i in 1:5) {
  model3_int1poly <- glm(log(M3) ~ poly(St, 10) + Re_categorical + Fr_categorical + Re_categorical * Fr_categorical, data = train1)
  cv_error_5_3poly[i] = cv.glm(train1, model3_int1poly, K = 5)$delta[2]
  }
sum(cv_error_5_3poly) / 5 # MSE
```

```{r}
best_poly <- rep(0, length(1:30))
for (j in 1:30) {
set.seed(j)
cv_rss = rep(NA, 10)
for (i in 1:10) {
    glm.fit = glm(log(M3) ~ poly(St, i) + Fr_categorical + Re_categorical + Re_categorical * Fr_categorical, data = train1)
    cv_rss[i] = cv.glm(train1, glm.fit, K = 5)$delta[2]
}

poly <- c(1:10)
cv_df <- data.frame(poly, cv_rss)

best_poly[j] <- which.min(x = cv_df$cv_rss)
}
summary(best_poly)
hist(best_poly)
```

Like M3, the spline model (with about 22 degrees of freedom) produces the lowest CV error, at about 1.4 This is slightly lower than the polynomial of degree 10 (CV error of 1.6) and much lower than the basic linear model with one interaction term (CV error of 4.4). 

## M4
```{r}
# Model 4 (linear)
model4 <- lm(log(M4) ~ St + Re_categorical + Fr_categorical, data = train1)
summary(model4)

model4_int1 <- lm(log(M4) ~ St + Re_categorical + Fr_categorical + 
                    Re_categorical * Fr_categorical, data = train1)
summary(model4_int1)
```


# Nonlinear models

```{r}
cv_rss = rep(NA, 10)
for (i in 1:10) {
    glm.fit = glm(log(M1) ~ poly(St, i) + Fr_categorical + Re_categorical, data = train1)
    cv_rss[i] = cv.glm(train1, glm.fit, K = 5)$delta[2]
}

poly <- c(1:10)
cv_df <- data.frame(poly, cv_rss)

cv_df

ggplot(cv_df, aes(x = poly, y = cv_rss)) +
  geom_line() +
  scale_x_continuous(breaks = c(1:10)) +
  labs(title = "CV Error",
       y = "CV Error",
       x = "Polynomial Degree") +
  theme_minimal(base_size = 11)
```

### Repeat for 30 seeds to see most common optimal polynomial degree
```{r}
best_poly <- rep(0, length(1:30))
for (j in 1:30) {
set.seed(j)
cv_rss = rep(NA, 10)
for (i in 1:10) {
    glm.fit = glm(log(M1) ~ poly(St, i) + Fr_categorical + Re_categorical, data = train1)
    cv_rss[i] = cv.glm(train1, glm.fit, K = 5)$delta[2]
}

poly <- c(1:10)
cv_df <- data.frame(poly, cv_rss)

# ggplot(cv_df, aes(x = poly, y = cv_rss)) +
#   geom_line() +
#   scale_x_continuous(breaks = c(1:10)) +
#   labs(title = "CV Error",
#        y = "CV Error",
#        x = "Polynomial Degree") +
#   theme_minimal(base_size = 11)

best_poly[j] <- which.min(x = cv_df$cv_rss)
}
summary(best_poly)
hist(best_poly)
```

## Spline M1
```{r warning = FALSE}
df_range_cv = rep(NA, 25)
for (i in 3:25) {
    lm.fit = gam(log(M1) ~ s(St, df = i) + Re_categorical + Fr_categorical, data = train1)
    df_range_cv[i] = cv.glm(train1, lm.fit, K = 5)$delta[2]
}

spline_df_cv <- c(3:25)
rss_cv <- df_range_cv[-c(1, 2)]
spline_rss_cv <- data.frame(spline_df_cv, rss_cv)
spline_rss_cv

ggplot(spline_rss_cv, aes(x = spline_df_cv, y = rss_cv)) +
  geom_line() +
  scale_x_continuous() +
  labs(title = "Spline",
       y = "CV Error",
       x = "df") +
  theme_minimal(base_size = 11)
```

## Spline M2
```{r}
df_range_cv = rep(NA, 25)
for (i in 3:25) {
    lm.fit = gam(log(M2) ~ s(St, df = i) + Re_categorical + Fr_categorical + 
                   Re_categorical * Fr_categorical, data = train1)
    df_range_cv[i] = cv.glm(train1, lm.fit, K = 5)$delta[2]
}

spline_df_cv <- c(3:25)
rss_cv <- df_range_cv[-c(1, 2)]
spline_rss_cv <- data.frame(spline_df_cv, rss_cv)
spline_rss_cv

ggplot(spline_rss_cv, aes(x = spline_df_cv, y = rss_cv)) +
  geom_line() +
  scale_x_continuous() +
  labs(title = "Spline",
       y = "CV Error",
       x = "df") +
  theme_minimal(base_size = 11)
```

## Spline M3
```{r}
df_range_cv = rep(NA, 25)
for (i in 3:25) {
    lm.fit = gam(log(M3) ~ s(St, df = i) + Re_categorical + Fr_categorical + 
                   Re_categorical * Fr_categorical, data = train1)
    df_range_cv[i] = cv.glm(train1, lm.fit, K = 5)$delta[2]
}

spline_df_cv <- c(3:25)
rss_cv <- df_range_cv[-c(1, 2)]
spline_rss_cv <- data.frame(spline_df_cv, rss_cv)
spline_rss_cv

ggplot(spline_rss_cv, aes(x = spline_df_cv, y = rss_cv)) +
  geom_line() +
  scale_x_continuous() +
  labs(title = "Spline",
       y = "CV Error",
       x = "df") +
  theme_minimal(base_size = 11)
```

## Apply to test data

```{r test-data}

```

